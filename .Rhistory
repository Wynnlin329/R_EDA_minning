install.packages(c("mice", "missForest", "VIM"))
install.packages("mice")#安裝 mice 套件
install.packages(c("mice", "missForest", "VIM"))
# 準備資料
data(chickwts)
str(chickwts)
table(chickwts$feed)
#將資料 by feed分成6小群
splt1 = split(chickwts$weight, chickwts$feed)
splt1
install.packages('C50')
library(C50)
?(churn)
data(churn)
data(churn)
str(churnTrain)
?(churn)
?(churn)
library(C50)
?(churn)
data(churn)
install.packages('C50')
install.packages("C50")
library(C50)
library(C50)
?(churn)
data(churn)
str(churnTrain)
install.packages('C50')
install.packages("C50")
library(C50)
?(churn)
data(churn)
str(churnTrain)
install.packages("C50", repos="http://R-Forge.R-project.org")
install.packages("C50", repos = "http://R-Forge.R-project.org")
library(C50)
library(C50)
?(churn)
data(churn)
?(churn)
data(churn)
str(churnTrain)
install.packages('C50')
install.packages("C50")
install.unpackages('C50')
uninstall.packages('C50')
remove.packages("C50")
install.packages('C50')
install.packages("C50")
library(C50)
?(churn)
data(churn)
install.packages('Rtools')
library(C50)
install.packages('C50')
install.packages("C50")
library(C50)
install.packages('C50')
install.packages("C50")
detach("package:base", unload = TRUE)
library(base)
library(C50)
install.packages('C50')
install.packages("C50")
library(C50)
?(churn)
data(churn)
str(churnTrain)
remove.packages('C50')
install.packages('C50')
install.packages("C50")
library(C50)
?(churn)
data(churn)
#餘弦相似度 (COSINE)
#下面的數據實際上是故意選擇了一個完美的線性關係：y = 0.10 + 0.01 x
# 所以相關係數 = 1
x <- c(1,2,3,5,8)
y <- c(0.11,0.12,0.13,0.15,0.18)
a <- matrix(c(x,y),c(5,2)) #5*2矩陣
a
#交叉分析表 ftable
z <- data.frame(Main= c('豚','牛','牛','牛','豚','牛','豚'),
sub=c('有','沒有','沒有','有','有','有','沒有') ,
drink=c('tea','coffee','coffee','tea','coffee','tea','coffee'))
z
ftable(z, row.vars = 1:2, col.vars = "drink")
ftable(z, row.vars = "Main", col.vars = "drink")
library(plyr)
# 產生40筆隨機資料
df <- data.frame(
group = c(rep('個人戶', 20), rep('企業戶', 20)),
sex = sample(c("M", "F"), size = 40, replace = TRUE),
age = floor(runif(n = 40, min = 25, max = 40)),
bill = floor(runif(n = 40, min = 199, max = 2600))
)
# 利用group, sex進行分組，並計算年齡的平均數、標準差以及bill總和與平均
ddply(df, .(group, sex), summarize,
mean_age = round(mean(age), 2),
sd_age = round(sd(age), 2),
sum_bill = sum(bill),
mean_bill = round(mean(bill), 2)
)
#計算資料筆數count
ddply(df, c('group','sex'), nrow)
ddply(df, c('group','sex','age'), nrow) #是不是很像樞紐分析表的原始資料
library(vcd)
#用vcd計算列聯表百分比
install.packages("vcd")
install.packages("vcd")
library(vcd)
#用vcd計算列聯表百分比
install.packages("vcd")
install.packages("vcd")
# 一次安裝所有packages
packages <- c("C50","tree", "rpart","randomForest")
for (i in packages){  install.packages(i) }
# 一次安裝所有packages
packages <- c("C50","tree", "rpart","randomForest")
for (i in packages){  install.packages(i) }
#一次載入packages
sapply(packages, FUN = library, character.only = TRUE)
search()
#訓練樣本70%, 測試樣本30%
install.packages("caret")
library(caret)#caret 可以讓你在切樣本的時候，特徵不會變
sample_Index <- createDataPartition(y=iris$Species,p=0.7,list=FALSE)#list:清單格式
iris.train=iris[sample_Index,]
iris.test=iris[-sample_Index,]
#確認訓練樣本與測試樣本分佈一致
par(mfrow=c(1,2))
#讓R的繪圖視窗切割成 1 X 2 的方塊
plot(iris.train$Species)
plot(iris.test$Species)
#確認訓練樣本與測試樣本分佈一致
par(mfrow=c(1,1))
#讓R的繪圖視窗切割成 1 X 2 的方塊
plot(iris.train$Species)
plot(iris.test$Species)
#模型訓練
iris.C50tree=C5.0(Species~ . ,data=iris.train)
summary(iris.C50tree)
plot(iris.C50tree)
plot(iris.C50tree)
plot(iris.C50tree)
#模型訓練
iris.C50tree=C5.0(Species~ . ,data=iris.train)
summary(iris.C50tree)
plot(iris.C50tree)
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
y = iris$Species[sample_Index]
y_hat= predict(iris.C50tree,iris.train,type='class')
table.train=table(y,y_hat)
cat("Total records(train)=",nrow(iris.train),"\n")
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=",
sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = iris$Species[-sample_Index]
y_hat= predict(iris.C50tree,iris.test,type='class')#class:文字
table.test=table(y,y_hat)
cat("Total records(test)=",nrow(iris.test),"\n")
cat("Correct Classification Ratio(test)=",
sum(diag(table.test))/sum(table.test)*100,"%\n")
#模型訓練
iris.RFtree = randomForest(Species ~ ., data=iris.train, importane=T, proximity =TRUE, ntree=300)
#importane:T 顯示重要變數,proximity =TRUE 預測品種的機率值,ntree=300 300棵樹(抽樣次數,看資料多少)
print(iris.RFtree )
#變數重要性
(round(importance(iris.RFtree ),2))
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
table.rf=iris.RFtree$confusion
cat("CORRECTION RATIO(train)=", sum(diag(table.rf)/sum(table.rf))*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = iris$Species[-sample_Index]
y_hat = predict(iris.RFtree ,newdata=iris.test)
table.test=table(y,y_hat)
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
#模型訓練
iris.RFtree = randomForest(Species ~ ., data=iris.train, importane=T, proximity =TRUE, ntree=100)
#importane:T 顯示重要變數,proximity =TRUE 預測品種的機率值,ntree=300 300棵樹(抽樣次數,看資料多少)
print(iris.RFtree )
#變數重要性
(round(importance(iris.RFtree ),2))
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
table.rf=iris.RFtree$confusion
cat("CORRECTION RATIO(train)=", sum(diag(table.rf)/sum(table.rf))*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = iris$Species[-sample_Index]
y_hat = predict(iris.RFtree ,newdata=iris.test)
table.test=table(y,y_hat)
cat("Correct Classification Ratio(test)=", sum(diag(table.test))/sum(table.test)*100,"%\n")
#分群模型
(iris.clutRF=randomForest(iris[,-5]))
#繪圖 Multi-Dimension plot
MDSplot(iris.clutRF,iris$Species)
churn <- read.csv("C:\\Program Files\\R\\R-3.6.2\\library\\C50\\data\\churn.csv", header=T)
churnTrain <- read.csv("C:\\Program Files\\R\\R-3.6.2\\library\\C50\\data\\churnTrain.csv", header=T)
data(churn)
churn <- read.csv("C:\\Program Files\\R\\R-3.6.2\\library\\C50\\data\\churn.csv", header=T)
churnTrain <- read.csv("C:\\Program Files\\R\\R-3.6.2\\library\\C50\\data\\churnTrain.csv", header=T)
churn <- read.csv("C:\\Program Files\\R\\R-3.6.2\\library\\C50\\data\\churn.csv", header=T)
churnTrain <- read.csv("C:\\Program Files\\R\\R-3.6.2\\library\\C50\\data\\churnTrain.csv", header=T)
churn <- read.csv("C:\\Users\\summe\\OneDrive\\文件\\R\\DM\\data\\churn.csv", header=T)
churnTrain <- read.csv("C:\\Users\\summe\\OneDrive\\文件\\R\\DM\\data\\churnTrain.csv", header=T)
#模型訓練
data_train = churnTrain[,-3]  # 排除 "area_code"欄位
churn.tree=rpart(churn~.,data=data_train)
churn.tree
# 繪製CART決策樹
plot(churn.tree)
text(churn.tree,cex = .8) #cex表示字體大小
# 變數重要性
churn.tree$variable.importance
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
y = churnTrain$churn
y_hat=predict(churn.tree,newdata=churnTrain,type="class")
table.train=table(y,y_hat)
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#測試樣本的混淆矩陣(confusion matrix)與預測正確率
y = churnTest$churn
y_hat=predict(churn.tree,newdata=churnTest,type="class")
#模型訓練
data_train = churnTrain[,-3,-8,-9,-11,-12,-15,-14,-17,-18]  # 排除 "area_code"欄位
churn.tree=rpart(churn~.,data=data_train)
#模型訓練
data_train = churnTrain[,-3-8-9-11-12-15-14-17-18]  # 排除 "area_code"欄位
churn.tree=rpart(churn~.,data=data_train)
churn.tree
# 繪製CART決策樹
plot(churn.tree)
text(churn.tree,cex = .8) #cex表示字體大小
# 變數重要性
churn.tree$variable.importance
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
y = churnTrain$churn
y_hat=predict(churn.tree,newdata=churnTrain,type="class")
table.train=table(y,y_hat)
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
install.packages("InformationValue") # for optimalCutoff
library(InformationValue)
y = data_train$churn
data_train = churnTrain[,-3] # 排除 "area_code"欄位
data_train = churnTrain[,-1] # 排除 “state"欄位
data_train$churn = ifelse(data_train$churn=='yes',1,0)  # 羅吉斯回歸預設對 y=1 建模產出推估機率
#模型訓練
logitM=glm(formula=churn~., data= data_train, family=binomial(link="logit"), na.action=na.exclude)
summary(logitM)
#訓練樣本的混淆矩陣(confusion matrix)與預測正確率
install.packages("InformationValue") # for optimalCutoff
library(InformationValue)
y = data_train$churn
y_hat=predict(logitM,newdata=churnTrain,type="response")
#optimalCutoff(y, y_hat)[1] 提供了找到最佳截止值，減少錯誤分類錯誤
table.train=table(y, y_hat > optimalCutoff(y, y_hat)[1] )
#預測正確率 = 矩陣對角對角總和 / 矩陣總和
cat("Correct Classification Ratio(train)=", sum(diag(table.train))/sum(table.train)*100,"%\n")
install.packages("ROCR")
install.packages("e1071")
library(e1071)
data <- read.csv("C:\\Users\\summe\\OneDrive\\文件\\R\\DM\\data\\churn.csv", header=T)
# 產生建模與測試樣本
n=0.3*nrow(data)
test.index=sample(1:nrow(data),n)
Train=data[-test.index,]
Test=data[test.index,]
# 建模
svm = svm(Species ~ . ,data=Train)
test.index=sample(1:nrow(data),n)
Train=data[-test.index,]
Test=data[test.index,]
# 建模
svm = svm(Species ~ . ,data=Train)
summary(svm)
data(iris)
data <- iris
# 產生建模與測試樣本
n=0.3*nrow(data)
test.index=sample(1:nrow(data),n)
Train=data[-test.index,]
Test=data[test.index,]
# 建模
svm = svm(Species ~ . ,data=Train)
summary(svm)
# 測試樣本預測正確率
Ypred = predict(svm, Test)
#  混淆矩陣 (預測率有93.33%)
message("準確度：",sum(diag(table(Test$Species,Ypred))) / sum(table(Test$Species,Ypred)) *100,"%")
iris_new <- iris[, -5]
set.seed(123)
Cluster_km <- kmeans(iris_new, nstart=15,centers=3) # center就是設定群數
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 5]) ＃觀察分群結果與實際類別的差別
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 5])#觀察分群結果與實際類別的差別
setosa versicolor virginica
plot(iris_new $Petal.Width, iris_new $Petal.Length, col=Cluster_km$cluster)
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 4])#觀察分群結果與實際類別的差別
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 3])#觀察分群結果與實際類別的差別
plot(iris_new $Petal.Width, iris_new $Petal.Length, col=Cluster_km$cluster)
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 3])#觀察分群結果與實際類別的差別
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 6])#觀察分群結果與實際類別的差別
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 5])#觀察分群結果與實際類別的差別
Cluster_km
Cluster_km <- kmeans(iris_new, nstart=15,centers=4) # center就是設定群數
# nstart 是指重新隨意放 k 個中心點的次數, 一般建議 nstart >= 10
table(Cluster_km$cluster, iris[, 5])#觀察分群結果與實際類別的差別
plot(iris_new $Petal.Width, iris_new $Petal.Length, col=Cluster_km$cluster)
Cluster_km
#最佳的分群數 K 如何決定？
WSS_ratio <- rep(NA, times = 10)
for (k in 1:length(WSS_ratio))
{
Cluster_km <- kmeans(iris_new, nstart=15,centers=k)
WSS_ratio[k] <- Cluster_km$tot.withinss
}
#畫陡坡圖
plot(WSS_ratio, type="b", main = "陡坡圖")
